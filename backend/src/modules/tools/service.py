import zipfile
import io
import time
import os
from datetime import datetime
from typing import Dict, List
from collections import Counter

from ml.model import YOLOModel
from .schemas import SingleAnalysisResponse, BatchAnalysisResponse, ImageAnalysisResult, DetectionItem, AnalysisConfig, AnalysisResult

class ToolService:
    def __init__(self):
        self.model = YOLOModel()
        self.base_output_dir = "results"
        os.makedirs(self.base_output_dir, exist_ok=True)
        self.expected_tools_count = 11
    
    def convert_to_detection_items(self, detections_dict: List[Dict]) -> List[DetectionItem]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Å–ª–æ–≤–∞—Ä–∏ –¥–µ—Ç–µ–∫—Ü–∏–π –≤ –æ–±—ä–µ–∫—Ç—ã DetectionItem"""
        detection_items = []
        for detection in detections_dict:
            detection_item = DetectionItem(
                class_id=detection['class_id'],
                class_name=detection['class_name'],
                confidence=detection['confidence'],
                bbox=detection['bbox']
            )
            detection_items.append(detection_item)
        return detection_items
    
    def analyze_tool_completeness(self, detections: List[DetectionItem]) -> AnalysisResult:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ–ª–Ω–æ—Ç—É –Ω–∞–±–æ—Ä–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤"""
        detected_classes = set()
        class_counts = Counter()
        detected_tools_names = []
        
        for detection in detections:
            detected_classes.add(detection.class_id)
            class_counts[detection.class_id] += 1
            detected_tools_names.append(detection.class_name)
        
        # –í—Å–µ –æ–∂–∏–¥–∞–µ–º—ã–µ –∫–ª–∞—Å—Å—ã (0-10)
        expected_classes = set(range(self.expected_tools_count))
        
        missing_classes = expected_classes - detected_classes
        extra_classes = detected_classes - expected_classes
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã (–∫–ª–∞—Å—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –±–æ–ª—å—à–µ 1 —Ä–∞–∑–∞)
        duplicate_classes = {class_id for class_id, count in class_counts.items() if count > 1}
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º ID –∫–ª–∞—Å—Å–æ–≤ –≤ –Ω–∞–∑–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
        missing_tools = [self.model.class_names[class_id] for class_id in missing_classes]
        extra_tools = [self.model.class_names[class_id] for class_id in extra_classes]
        duplicate_tools = [self.model.class_names[class_id] for class_id in duplicate_classes]
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –∞–Ω–∞–ª–∏–∑–∞ —Å —É—á–µ—Ç–æ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        total_unique_classes = len(detected_classes)
        
        if total_unique_classes == self.expected_tools_count and not extra_classes and not duplicate_classes:
            status = "complete"
            message = "‚úÖ –ü–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä: –≤—Å–µ 11 –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã (–±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤)"
        elif total_unique_classes == self.expected_tools_count and not extra_classes and duplicate_classes:
            status = "duplicates"
            message = f"üîÑ –ü–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä —Å –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏: –≤—Å–µ 11 –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, –Ω–æ {len(duplicate_tools)} –¥—É–±–ª–∏—Ä—É—é—Ç—Å—è"
        elif missing_classes and not extra_classes and not duplicate_classes:
            status = "missing"
            message = f"‚ö†Ô∏è –ù–µ–ø–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç {len(missing_classes)} –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç(–æ–≤)"
        elif missing_classes and duplicate_classes and not extra_classes:
            status = "missing_duplicates"
            message = f"üîÑ –ù–µ–ø–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä —Å –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç {len(missing_tools)}, –¥—É–±–ª–∏—Ä—É–µ—Ç—Å—è {len(duplicate_tools)}"
        elif extra_classes and not missing_classes:
            status = "extra"
            message = f"‚ùå –õ–∏—à–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã: –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(extra_classes)} –ª–∏—à–Ω–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç(–æ–≤)"
        elif extra_classes and missing_classes:
            status = "mixed"
            message = f"üîÄ –°–º–µ—à–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç {len(missing_tools)}, –ª–∏—à–Ω–∏—Ö {len(extra_tools)}"
        elif duplicate_classes and not missing_classes and not extra_classes:
            status = "duplicates_only"
            message = f"üîÑ –î—É–±–ª–∏–∫–∞—Ç—ã: –≤—Å–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç, –Ω–æ {len(duplicate_tools)} –¥—É–±–ª–∏—Ä—É—é—Ç—Å—è"
        else:
            status = "unknown"
            message = f"‚ùì –ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {total_unique_classes} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤"
        
        # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã —Å extra_tools –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª—É—á–∞—è—Ö
        if status in ["duplicates", "duplicates_only", "missing_duplicates"]:
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥—É–±–ª–∏–∫–∞—Ç–∞—Ö –≤ extra_tools –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–µ
            extra_tools.extend([f"{tool} (–¥—É–±–ª–∏–∫–∞—Ç)" for tool in duplicate_tools])
        
        return AnalysisResult(
            status=status,
            total_detections=len(detections),
            expected_count=self.expected_tools_count,
            missing_tools=missing_tools,
            extra_tools=extra_tools,
            detected_tools=detected_tools_names,
            detections=detections,
            message=message
        )
    
    def analyze_single_image(self, image_data: bytes, filename: str = "image", 
                           confidence_threshold: float = 0.25, iou_threshold: float = 0.45) -> SingleAnalysisResponse:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞ –ø–æ–ª–Ω–æ—Ç—É –Ω–∞–±–æ—Ä–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –º–æ–¥–µ–ª–∏ –∏ –∏—Å—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            detections_dict, original_image = self.model.predict(image_data, confidence_threshold, iou_threshold)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å–ª–æ–≤–∞—Ä–∏ –≤ –æ–±—ä–µ–∫—Ç—ã DetectionItem
            detection_items = self.convert_to_detection_items(detections_dict)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ–ª–Ω–æ—Ç—É –Ω–∞–±–æ—Ä–∞
            analysis_result = self.analyze_tool_completeness(detection_items)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            annotated_path = self.model.save_annotated_image(original_image, detections_dict, filename)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö
            config = AnalysisConfig(
                confidence_threshold=confidence_threshold,
                iou_threshold=iou_threshold,
                annotated_image_path=annotated_path
            )
            
            return SingleAnalysisResponse(
                status="success",
                analysis_result=analysis_result,
                config=config
            )
            
        except Exception as e:
            error_result = AnalysisResult(
                status="error",
                total_detections=0,
                message=f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}",
                detections=[]
            )
            
            return SingleAnalysisResponse(
                status="error",
                analysis_result=error_result,
                config=AnalysisConfig(
                    confidence_threshold=confidence_threshold,
                    iou_threshold=iou_threshold
                )
            )
    
    def analyze_batch_images(self, zip_data: bytes,
                           confidence_threshold: float = 0.25, iou_threshold: float = 0.45) -> BatchAnalysisResponse:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≥—Ä—É–ø–ø—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –ø–æ–ª–Ω–æ—Ç—É –Ω–∞–±–æ—Ä–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤"""
        start_time = time.time()
        
        try:
            zip_file = zipfile.ZipFile(io.BytesIO(zip_data))
            file_list = zip_file.namelist()
            
            image_extensions = {'.jpg', '.jpeg', '.png'}
            image_files = [
                f for f in file_list 
                if any(f.lower().endswith(ext) for ext in image_extensions)
            ]
            
            if not image_files:
                return BatchAnalysisResponse(
                    status="error",
                    total_images=0,
                    processed_images=0,
                    results=[],
                    processing_time=0,
                    summary={}
                )
            
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —ç—Ç–æ–π —Å–µ—Å—Å–∏–∏
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            session_dir = os.path.join(self.base_output_dir, f"batch_session_{timestamp}")
            os.makedirs(session_dir, exist_ok=True)
            
            results = []
            processed_count = 0
            status_summary = {
                "complete": 0,
                "missing": 0,
                "extra": 0,
                "mixed": 0,
                "duplicates": 0,
                "duplicates_only": 0,
                "missing_duplicates": 0,
                "error": 0
            }
            
            for filename in image_files:
                try:
                    with zip_file.open(filename) as image_file:
                        image_data = image_file.read()
                    
                    detections_dict, original_image = self.model.predict(image_data, confidence_threshold, iou_threshold)
                    
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å–ª–æ–≤–∞—Ä–∏ –≤ –æ–±—ä–µ–∫—Ç—ã DetectionItem
                    detection_items = self.convert_to_detection_items(detections_dict)
                    
                    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ–ª–Ω–æ—Ç—É –Ω–∞–±–æ—Ä–∞
                    analysis_result = self.analyze_tool_completeness(detection_items)
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    annotated_path = self.model.save_annotated_image(original_image, detections_dict, filename, session_dir)
                    
                    image_result = ImageAnalysisResult(
                        filename=filename,
                        analysis_result=analysis_result,
                        annotated_image_path=annotated_path
                    )
                    
                    results.append(image_result)
                    status_summary[analysis_result.status] += 1
                    processed_count += 1
                    
                    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {filename} - {analysis_result.status}")
                    
                except Exception as e:
                    error_result = AnalysisResult(
                        status="error",
                        total_detections=0,
                        message=f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}",
                        detections=[]
                    )
                    
                    error_image_result = ImageAnalysisResult(
                        filename=filename,
                        analysis_result=error_result
                    )
                    
                    results.append(error_image_result)
                    status_summary["error"] += 1
                    processed_count += 1
            
            processing_time = time.time() - start_time
            
            config = AnalysisConfig(
                confidence_threshold=confidence_threshold,
                iou_threshold=iou_threshold,
                output_directory=session_dir,
                total_annotated_images=len([r for r in results if r.annotated_image_path])
            )
            
            return BatchAnalysisResponse(
                status="completed",
                total_images=len(image_files),
                processed_images=processed_count,
                results=results,
                processing_time=round(processing_time, 2),
                summary=status_summary,
                config=config
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            return BatchAnalysisResponse(
                status="error",
                total_images=0,
                processed_images=0,
                results=[],
                processing_time=round(processing_time, 2),
                summary={}
            )

tool_service = ToolService()